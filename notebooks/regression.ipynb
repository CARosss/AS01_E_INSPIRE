{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-26T12:12:47.789836Z",
     "start_time": "2025-01-26T12:12:47.786629Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    make_scorer\n",
    ")\n",
    "from sklearn.base import clone\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n"
   ],
   "outputs": [],
   "execution_count": 302
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T12:12:47.809320Z",
     "start_time": "2025-01-26T12:12:47.798699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('../data/E-INSPIRE_I_master_catalogue.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFeature Information:\")\n",
    "df.info()\n",
    "\n",
    "# Show first few rows\n",
    "df.head()\n",
    "\n",
    "\n",
    "np.random.seed(42)\n"
   ],
   "id": "20b2398c970b711c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (430, 75)\n",
      "\n",
      "Feature Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 430 entries, 0 to 429\n",
      "Data columns (total 75 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   GALAXY ID              430 non-null    object \n",
      " 1   ra                     430 non-null    float64\n",
      " 2   dec                    430 non-null    float64\n",
      " 3   plate                  430 non-null    int64  \n",
      " 4   mjd                    430 non-null    int64  \n",
      " 5   fiberid                430 non-null    int64  \n",
      " 6   objid                  430 non-null    float64\n",
      " 7   deVRad_r               430 non-null    float64\n",
      " 8   deVRadErr_r            430 non-null    float64\n",
      " 9   expRad_r               430 non-null    float64\n",
      " 10  expRadErr_r            430 non-null    float64\n",
      " 11  z                      430 non-null    float64\n",
      " 12  zErr                   430 non-null    float64\n",
      " 13  SNR                    430 non-null    float64\n",
      " 14  velDisp_SDSS           430 non-null    float64\n",
      " 15  velDispErr_SDSS        430 non-null    float64\n",
      " 16  velDisp_ppxf_full      430 non-null    float64\n",
      " 17  velDisp_ppxf_err_full  430 non-null    float64\n",
      " 18  velDisp_ppxf_res       430 non-null    float64\n",
      " 19  velDisp_ppxf_err_res   430 non-null    float64\n",
      " 20  logM*                  430 non-null    float64\n",
      " 21  errlogM*               430 non-null    float64\n",
      " 22  logSFR                 430 non-null    float64\n",
      " 23  errlogSFR              430 non-null    float64\n",
      " 24  logsSFR                430 non-null    float64\n",
      " 25  errlogsSFR             430 non-null    float64\n",
      " 26  petroR50Errkpc_r       430 non-null    float64\n",
      " 27  petroR50kpc_r          430 non-null    float64\n",
      " 28  deVRadkpc_r            430 non-null    float64\n",
      " 29  deVRadErrkpc_r         430 non-null    float64\n",
      " 30  expRadkpc_r            430 non-null    float64\n",
      " 31  expRadErrkpc_r         430 non-null    float64\n",
      " 32  fracDev_r              430 non-null    float64\n",
      " 33  meanRad_r              430 non-null    float64\n",
      " 34  meanRadErr_r           430 non-null    float64\n",
      " 35  meanRadkpc_r           430 non-null    float64\n",
      " 36  meanRadErrkpc_r        430 non-null    float64\n",
      " 37  meanRad_kids           430 non-null    float64\n",
      " 38  meanRadkpc_kids        430 non-null    float64\n",
      " 39  Mgb_ssp                430 non-null    float64\n",
      " 40  Fe_avg_ssp             430 non-null    float64\n",
      " 41  Mgb_sigmacorr          430 non-null    float64\n",
      " 42  Fe_avg_sigmacorr       430 non-null    float64\n",
      " 43  MgFe                   430 non-null    float64\n",
      " 44  alphaFe_flag           430 non-null    int64  \n",
      " 45  univ_age               430 non-null    float64\n",
      " 46  age_mean_mass          430 non-null    float64\n",
      " 47  age_err_mass           430 non-null    float64\n",
      " 48  age_mean_light         430 non-null    float64\n",
      " 49  age_err_light          430 non-null    float64\n",
      " 50  [M/H]_mean_mass        430 non-null    float64\n",
      " 51  [M/H]_err_mass         430 non-null    float64\n",
      " 52  [M/H]_mean_light       430 non-null    float64\n",
      " 53  [M/H]_err_light        430 non-null    float64\n",
      " 54  Mtbb3                  430 non-null    float64\n",
      " 55  Mtbb3_unr              430 non-null    float64\n",
      " 56  Mtbb3_reg              430 non-null    float64\n",
      " 57  Mtbb3_alphaplus        320 non-null    float64\n",
      " 58  Mtbb3_alphamin         373 non-null    float64\n",
      " 59  t_75                   430 non-null    float64\n",
      " 60  t_75_unr               430 non-null    float64\n",
      " 61  t_75_reg               430 non-null    float64\n",
      " 62  t_75_alphaplus         320 non-null    float64\n",
      " 63  t_75_alphamin          373 non-null    float64\n",
      " 64  t_90                   430 non-null    float64\n",
      " 65  t_90_unr               430 non-null    float64\n",
      " 66  t_90_reg               430 non-null    float64\n",
      " 67  t_90_alphaplus         320 non-null    float64\n",
      " 68  t_90_alphamin          373 non-null    float64\n",
      " 69  t_100                  430 non-null    float64\n",
      " 70  t_100_unr              430 non-null    float64\n",
      " 71  t_100_reg              430 non-null    float64\n",
      " 72  t_100_alphaplus        320 non-null    float64\n",
      " 73  t_100_alphamin         373 non-null    float64\n",
      " 74  DoR                    430 non-null    float64\n",
      "dtypes: float64(70), int64(4), object(1)\n",
      "memory usage: 252.1+ KB\n"
     ]
    }
   ],
   "execution_count": 303
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T12:12:47.842525Z",
     "start_time": "2025-01-26T12:12:47.838792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_data(df, selected_features, log_transform_features=None, one_hot_encode=False):\n",
    "    X = df[selected_features].copy()\n",
    "    \n",
    "    # Store original MgFe for potential restoration\n",
    "    original_mgfe = X['MgFe'].copy() if 'MgFe' in X.columns else None\n",
    "    \n",
    "    if one_hot_encode and 'MgFe' in X.columns:\n",
    "        # Define bins and labels\n",
    "        bins = [0.0, 0.1, 0.2, 0.3, 0.4, np.inf]\n",
    "        mgfe_labels = ['MgFe_0.0', 'MgFe_0.1', 'MgFe_0.2', 'MgFe_0.3', 'MgFe_0.4']\n",
    "        \n",
    "        # Create binned version\n",
    "        X['MgFe_binned'] = pd.cut(X['MgFe'], \n",
    "                                 bins=bins, \n",
    "                                 labels=mgfe_labels, \n",
    "                                 include_lowest=True, \n",
    "                                 right=False)\n",
    "        \n",
    "        # One-hot encode the binned column\n",
    "        mgfe_encoded = pd.get_dummies(X['MgFe_binned'], prefix='', prefix_sep='')\n",
    "        \n",
    "        # Drop original and binned MgFe columns\n",
    "        X = X.drop(['MgFe', 'MgFe_binned'], axis=1)\n",
    "        \n",
    "        # Add encoded columns\n",
    "        X = pd.concat([X, mgfe_encoded], axis=1)\n",
    "    \n",
    "    # Log transform features\n",
    "    if log_transform_features is None:\n",
    "        log_transform_features = {\n",
    "            'age_mean_mass': False,\n",
    "            'velDisp_ppxf_res': False,\n",
    "            '[M/H]_mean_mass': False\n",
    "        }\n",
    "\n",
    "    for feature, do_log in log_transform_features.items():\n",
    "        if do_log and feature in X.columns:\n",
    "            X[feature] = np.log10(X[feature] + 1e-10)\n",
    "    \n",
    "    return X, X.columns.tolist(), original_mgfe\n",
    "\n",
    "def restore_original_mgfe(df, original_mgfe, mgfe_labels=None):\n",
    "    if mgfe_labels is None:\n",
    "        mgfe_labels = ['MgFe_0.0', 'MgFe_0.1', 'MgFe_0.2', 'MgFe_0.3', 'MgFe_0.4']\n",
    "    \n",
    "    # Remove one-hot columns\n",
    "    df = df.drop(columns=[col for col in df.columns if col in mgfe_labels])\n",
    "    \n",
    "    # Restore original MgFe\n",
    "    if original_mgfe is not None:\n",
    "        df['MgFe'] = original_mgfe\n",
    "    \n",
    "    return df"
   ],
   "id": "bf20fd7fe47a0a3",
   "outputs": [],
   "execution_count": 304
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T12:12:47.871449Z",
     "start_time": "2025-01-26T12:12:47.867682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_feature_importance(model, feature_names):\n",
    "    \"\"\"Plot feature importances for models that support them.\"\"\"\n",
    "    if hasattr(model.named_steps['regressor'], 'feature_importances_'):\n",
    "        importance = model.named_steps['regressor'].feature_importances_\n",
    "        indices = np.argsort(importance)[::-1]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.bar(range(len(importance)), importance[indices])\n",
    "        plt.xticks(range(len(importance)), [feature_names[i] for i in indices], rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print numerical values\n",
    "        for i in indices:\n",
    "            print(f\"{feature_names[i]}: {importance[i]:.4f}\")\n",
    "    else:\n",
    "        print(\"This model doesn't support feature importances\")\n",
    "\n",
    "def plot_regression_results(y_true, y_pred, model_name):\n",
    "    \"\"\"Plot regression results with metrics.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.scatter(y_true, y_pred, alpha=0.5)\n",
    "\n",
    "    # Diagonal line for perfect predictions\n",
    "    min_val, max_val = 0, 1\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    ax.set_title(f'{model_name} Regression Results')\n",
    "    ax.set_xlabel('True Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "\n",
    "    text = (f'R² = {r2:.3f}\\n'\n",
    "            f'RMSE = {rmse:.3f}\\n'\n",
    "            f'MAE = {mae:.3f}')\n",
    "    ax.text(0.05, 0.95, text, transform=ax.transAxes,\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "\n",
    "    return fig"
   ],
   "id": "2dd3bb4951ab816f",
   "outputs": [],
   "execution_count": 305
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T12:12:47.901742Z",
     "start_time": "2025-01-26T12:12:47.896266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def comprehensive_regression_analysis(X, y, test_size=0.2, random_state=42, log_transform_features=None):\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model_info in models.items():\n",
    "        print(f\"\\nEvaluating {name}...\")\n",
    "        \n",
    "        # Create and fit scaler\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        if model_info['params']:\n",
    "            best_score = float('-inf')\n",
    "            best_params = None\n",
    "            best_model = None\n",
    "            \n",
    "            # Manual grid search for XGBoost and Random Forest\n",
    "            if name in ['XGBoost', 'Random Forest']:\n",
    "                from itertools import product\n",
    "                param_combinations = [dict(zip(model_info['params'].keys(), v)) \n",
    "                                   for v in product(*model_info['params'].values())]\n",
    "                \n",
    "                for params in param_combinations:\n",
    "                    # Extract random_state from params\n",
    "                    rand_state = params.pop('random_state', 42)\n",
    "                    \n",
    "                    if name == 'XGBoost':\n",
    "                        model = XGBRegressor(random_state=rand_state, objective='reg:squarederror', **params)\n",
    "                    else:  # Random Forest\n",
    "                        model = RandomForestRegressor(random_state=rand_state, **params)\n",
    "                    \n",
    "                    # Put random_state back in params for recording purposes\n",
    "                    params['random_state'] = rand_state\n",
    "                    \n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "                    score = -mean_squared_error(y_train, model.predict(X_train_scaled))\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params = params\n",
    "                        best_model = model\n",
    "            else:\n",
    "                # Use GridSearchCV for other models\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=model_info['model'],\n",
    "                    param_grid=model_info['params'],\n",
    "                    cv=5,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    n_jobs=1\n",
    "                )\n",
    "                grid_search.fit(X_train_scaled, y_train)\n",
    "                best_model = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "            \n",
    "            y_pred = best_model.predict(X_test_scaled)\n",
    "            \n",
    "            results[name] = {\n",
    "                'best_params': best_params,\n",
    "                'y_pred': y_pred,\n",
    "                'r2_score': r2_score(y_test, y_pred),\n",
    "                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                'mae': mean_absolute_error(y_test, y_pred)\n",
    "            }\n",
    "            \n",
    "            print(\"Best Parameters:\", best_params)\n",
    "            \n",
    "            if hasattr(best_model, 'feature_importances_'):\n",
    "                importance = best_model.feature_importances_\n",
    "                indices = np.argsort(importance)[::-1]\n",
    "                \n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.title(\"Feature Importances\")\n",
    "                plt.bar(range(len(importance)), importance[indices])\n",
    "                plt.xticks(range(len(importance)), [X.columns[i] for i in indices], rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                for i in indices:\n",
    "                    print(f\"{X.columns[i]}: {importance[i]:.4f}\")\n",
    "        else:\n",
    "            # For models without parameters (like Linear Regression)\n",
    "            model = clone(model_info['model'])\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            results[name] = {\n",
    "                'y_pred': y_pred,\n",
    "                'r2_score': r2_score(y_test, y_pred),\n",
    "                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                'mae': mean_absolute_error(y_test, y_pred)\n",
    "            }\n",
    "        \n",
    "        # Visualization\n",
    "        plot_regression_results(y_test, results[name]['y_pred'], name)\n",
    "        plt.show()\n",
    "    \n",
    "    return results"
   ],
   "id": "db34012cb8e0b7ff",
   "outputs": [],
   "execution_count": 306
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T12:12:47.929444Z",
     "start_time": "2025-01-26T12:12:47.926680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = {\n",
    "    'XGBoost': {\n",
    "        'model': XGBRegressor(objective='reg:squarederror'),\n",
    "        'params': {\n",
    "            'n_estimators': [300, 350, 400],       # back to lower values\n",
    "            'max_depth': [7, 8, 9],                # slightly lower\n",
    "            'learning_rate': [0.2, 0.3, 0.4],      # back to lower values\n",
    "            'subsample': [0.5, 0.6, 0.7],          # increase from 0.4\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': [125, 150, 175],       # around 150\n",
    "            'max_depth': [15, 20, 25],             # around 20\n",
    "            'min_samples_split': [2],              # keep at 2 since it's consistently best\n",
    "            'max_features': [0.4, 0.5, 0.6],       # around 0.5\n",
    "            'max_samples': [0.9, 0.95, 1.0],       # fine-tune near 1.0\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    'Linear Regression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}\n",
    "    },\n",
    "}"
   ],
   "id": "37e6be9371f37c1",
   "outputs": [],
   "execution_count": 307
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T12:12:51.978309Z",
     "start_time": "2025-01-26T12:12:47.955995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select features for the analysis\n",
    "selected_features = ['MgFe', '[M/H]_mean_mass', 'velDisp_ppxf_res', 'age_mean_mass']\n",
    "\n",
    "# Configure log transformations\n",
    "log_transform_config = {\n",
    "    'age_mean_mass': True,\n",
    "    'velDisp_ppxf_res': False,\n",
    "    'MgFe': False,\n",
    "    '[M/H]_mean_mass': False\n",
    "}\n",
    "\n",
    "# Preprocess data\n",
    "one_hot_encode = True  # Set this to False if you don't want one-hot encoding\n",
    "\n",
    "# Preprocess data\n",
    "X, feature_names, original_mgfe = preprocess_data(\n",
    "    df, \n",
    "    selected_features, \n",
    "    log_transform_features=log_transform_config,\n",
    "    one_hot_encode=one_hot_encode\n",
    ")\n",
    "y = df['DoR'].values\n",
    "\n",
    "# Run the analysis\n",
    "results = comprehensive_regression_analysis(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    log_transform_features=log_transform_config\n",
    ")\n",
    "\n",
    "# Print final results comparison\n",
    "print(\"\\n--- Regression Model Comparison ---\")\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"R² Score: {result['r2_score']:.4f}\")\n",
    "    print(f\"RMSE: {result['rmse']:.4f}\")\n",
    "    print(f\"MAE: {result['mae']:.4f}\")\n",
    "    if 'best_params' in result:\n",
    "        print(\"Best Hyperparameters:\", result['best_params'])\n",
    "        \n",
    "if one_hot_encode:\n",
    "    X = restore_original_mgfe(X, original_mgfe)"
   ],
   "id": "2b659986bda91d68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating XGBoost...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[308], line 25\u001B[0m\n\u001B[1;32m     22\u001B[0m y \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDoR\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Run the analysis\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mcomprehensive_regression_analysis\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlog_transform_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_transform_config\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# Print final results comparison\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m--- Regression Model Comparison ---\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[306], line 40\u001B[0m, in \u001B[0;36mcomprehensive_regression_analysis\u001B[0;34m(X, y, test_size, random_state, log_transform_features)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# Put random_state back in params for recording purposes\u001B[39;00m\n\u001B[1;32m     38\u001B[0m params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrandom_state\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m rand_state\n\u001B[0;32m---> 40\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m score \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mmean_squared_error(y_train, model\u001B[38;5;241m.\u001B[39mpredict(X_train_scaled))\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m score \u001B[38;5;241m>\u001B[39m best_score:\n",
      "File \u001B[0;32m~/Documents/PythonPlay/AS01_E_INSPIRE/.venv/lib/python3.11/site-packages/xgboost/core.py:726\u001B[0m, in \u001B[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[1;32m    725\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[0;32m--> 726\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/PythonPlay/AS01_E_INSPIRE/.venv/lib/python3.11/site-packages/xgboost/sklearn.py:1108\u001B[0m, in \u001B[0;36mXGBModel.fit\u001B[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001B[0m\n\u001B[1;32m   1105\u001B[0m     obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m model, metric, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_configure_fit(xgb_model, params)\n\u001B[0;32m-> 1108\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_Booster \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1109\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1110\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dmatrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_num_boosting_rounds\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1112\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1113\u001B[0m \u001B[43m    \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1114\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals_result\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals_result\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1115\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1116\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcustom_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1117\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1118\u001B[0m \u001B[43m    \u001B[49m\u001B[43mxgb_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1119\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1120\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_evaluation_result(evals_result)\n\u001B[1;32m   1123\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/Documents/PythonPlay/AS01_E_INSPIRE/.venv/lib/python3.11/site-packages/xgboost/core.py:726\u001B[0m, in \u001B[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[1;32m    725\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[0;32m--> 726\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/PythonPlay/AS01_E_INSPIRE/.venv/lib/python3.11/site-packages/xgboost/training.py:181\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cb_container\u001B[38;5;241m.\u001B[39mbefore_iteration(bst, i, dtrain, evals):\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m--> 181\u001B[0m \u001B[43mbst\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miteration\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cb_container\u001B[38;5;241m.\u001B[39mafter_iteration(bst, i, dtrain, evals):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/PythonPlay/AS01_E_INSPIRE/.venv/lib/python3.11/site-packages/xgboost/core.py:2101\u001B[0m, in \u001B[0;36mBooster.update\u001B[0;34m(self, dtrain, iteration, fobj)\u001B[0m\n\u001B[1;32m   2097\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_dmatrix_features(dtrain)\n\u001B[1;32m   2099\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2100\u001B[0m     _check_call(\n\u001B[0;32m-> 2101\u001B[0m         \u001B[43m_LIB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mXGBoosterUpdateOneIter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2102\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\n\u001B[1;32m   2103\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2104\u001B[0m     )\n\u001B[1;32m   2105\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2106\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(dtrain, output_margin\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 308
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
